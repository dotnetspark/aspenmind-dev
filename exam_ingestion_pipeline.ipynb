{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50b636b",
   "metadata": {},
   "source": [
    "### üì¶ Step 1: Install required packages (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1e84cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: openai in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: azure-search-documents in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (11.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (1.37.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openai azure-search-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33ca29",
   "metadata": {},
   "source": [
    "### üîê Step 2: Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89a7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "AZURE_EXAM_INDEX = os.getenv(\"AZURE_EXAM_INDEX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb697f",
   "metadata": {},
   "source": [
    "### üì¶ Step 3: Upgrade azure-search-documents package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb6a4572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-search-documents in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (11.6.0)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (1.37.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-search-documents) (4.15.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ylrre\\source\\repos\\aspenmind-dev\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade azure-search-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65144d02",
   "metadata": {},
   "source": [
    "### üîÑ Step 4a: Updated Index Schema (with Quality Scoring fields)\n",
    "This schema extends the original with fields needed for the quality scoring feedback loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d260490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleted existing index: jdn-exam-sept2025-items\n",
      "‚úÖ Created index with quality + review/state management fields: jdn-exam-sept2025-items\n",
      "‚úÖ Created index with quality + review/state management fields: jdn-exam-sept2025-items\n"
     ]
    }
   ],
   "source": [
    "# Updated index schema with quality scoring + review/state management fields\n",
    "# WARNING: This will DELETE and RECREATE the existing index\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_KEY)\n",
    ")\n",
    "\n",
    "embedding_dimensions = 1536\n",
    "\n",
    "index_schema = SearchIndex(\n",
    "    name=AZURE_EXAM_INDEX,  # Using existing index name\n",
    "    fields=[\n",
    "        # === CORE FIELDS (existing) ===\n",
    "        SearchField(name=\"id\", type=SearchFieldDataType.String, key=True, searchable=False),\n",
    "        SearchField(name=\"domain\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "        SearchField(name=\"topic\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n",
    "        SearchField(name=\"evidence\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"question_text\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"options_raw\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"correct_answer\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchField(name=\"rationale\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"full_text\", type=SearchFieldDataType.String, searchable=True),\n",
    "        \n",
    "        # === Structured item components ===\n",
    "        SearchField(name=\"stimulus\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"stem\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"option_a\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"option_b\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"option_c\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"option_d\", type=SearchFieldDataType.String, searchable=True),\n",
    "        \n",
    "        # === Quality scoring fields ===\n",
    "        SearchField(name=\"quality_score\", type=SearchFieldDataType.Double, filterable=True, sortable=True),\n",
    "        SearchField(name=\"quality_tier\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchField(name=\"quality_summary\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"quality_scores_json\", type=SearchFieldDataType.String, searchable=False),  # JSON blob\n",
    "        SearchField(\n",
    "            name=\"improvement_suggestions\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "            searchable=True\n",
    "        ),\n",
    "        \n",
    "        # === REVIEW & STATE MANAGEMENT (Human-in-the-Loop) ===\n",
    "        SearchField(name=\"review_status\", type=SearchFieldDataType.String, filterable=True, facetable=True),  \n",
    "        # Values: \"gold_standard\" | \"pending_review\" | \"approved\" | \"approved_with_edits\" | \"rejected\"\n",
    "        SearchField(name=\"reviewed_at\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "        SearchField(name=\"reviewed_by\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchField(name=\"review_decision\", type=SearchFieldDataType.String, filterable=True),  # \"upvote\" | \"downvote\"\n",
    "        SearchField(name=\"review_explanation\", type=SearchFieldDataType.String, searchable=True),  # Why rejected/edited\n",
    "        \n",
    "        # === EDIT TRACKING ===\n",
    "        SearchField(name=\"was_edited\", type=SearchFieldDataType.Boolean, filterable=True),\n",
    "        SearchField(name=\"original_version_json\", type=SearchFieldDataType.String, searchable=False),  # JSON snapshot before edits\n",
    "        SearchField(name=\"edit_summary\", type=SearchFieldDataType.String, searchable=True),\n",
    "        \n",
    "        # === GENERATION METADATA (for Agent Framework + Analytics) ===\n",
    "        SearchField(name=\"generation_batch_id\", type=SearchFieldDataType.String, filterable=True),  # Group items from same batch\n",
    "        SearchField(name=\"generation_attempt\", type=SearchFieldDataType.Int32, filterable=True),  # Which retry generated this\n",
    "        SearchField(name=\"similarity_at_generation\", type=SearchFieldDataType.Double, filterable=True),  # Max similarity when generated\n",
    "        SearchField(name=\"generation_metadata_json\", type=SearchFieldDataType.String, searchable=False),  # Full generation context\n",
    "        \n",
    "        # === Metadata/tracking fields ===\n",
    "        SearchField(name=\"source\", type=SearchFieldDataType.String, filterable=True, facetable=True),  # \"original\" | \"generated_v2\"\n",
    "        SearchField(name=\"is_generated\", type=SearchFieldDataType.Boolean, filterable=True),\n",
    "        SearchField(name=\"created_at\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "        SearchField(name=\"scored_at\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "\n",
    "        # === VECTOR FIELD (existing) ===\n",
    "        SearchField(\n",
    "            name=\"content_vector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=embedding_dimensions,\n",
    "            vector_search_profile_name=\"myHnswProfile\"\n",
    "        )\n",
    "    ],\n",
    "    vector_search=VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=\"hnsw\"\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Delete existing index if it exists, then create with new schema\n",
    "if AZURE_EXAM_INDEX in index_client.list_index_names():\n",
    "    index_client.delete_index(AZURE_EXAM_INDEX)\n",
    "    print(f\"üóëÔ∏è Deleted existing index: {AZURE_EXAM_INDEX}\")\n",
    "\n",
    "index_client.create_index(index_schema)\n",
    "print(f\"‚úÖ Created index with quality + review/state management fields: {AZURE_EXAM_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca1920",
   "metadata": {},
   "source": [
    "### üèÜ Step 4b: Migrate Existing High-Quality Exams\n",
    "Your existing exams from 09/2025 are already high-quality (human-authored, gold standard). \n",
    "We'll mark them as **gold tier** with a perfect score to establish the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5862e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 24 rows from CSV\n",
      "‚úÖ Converted 24 documents with quality metadata\n",
      "\n",
      "Sample document quality fields:\n",
      "  - quality_score: 5.0\n",
      "  - quality_tier: gold\n",
      "  - review_status: gold_standard\n",
      "  - source: original\n",
      "  - is_generated: False\n"
     ]
    }
   ],
   "source": [
    "# Updated document converter that includes quality fields for EXISTING high-quality exams\n",
    "# These are your gold-standard items from 12/2025\n",
    "\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"./dataset/jdn-items-and-metadata_09-17-25_2.csv\", encoding='latin1')\n",
    "print(f\"üìÑ Loaded {len(df)} rows from CSV\")\n",
    "\n",
    "def row_to_doc_v2(row):\n",
    "    \"\"\"\n",
    "    Convert CSV row to document with quality metadata.\n",
    "    Existing items are marked as GOLD tier since they're human-authored gold standards.\n",
    "    \"\"\"\n",
    "    evidence = \"\\n\".join(filter(pd.notna, [\n",
    "        row.get(\"Evidence Statement #1\", \"\"),\n",
    "        row.get(\"Evidence Statement #2\", \"\"),\n",
    "        row.get(\"Evidence Statement #3\", \"\")\n",
    "    ]))\n",
    "    \n",
    "    # Build options dict and raw string\n",
    "    options = {}\n",
    "    options_lines = []\n",
    "    for opt in ['A', 'B', 'C', 'D', 'E']:\n",
    "        opt_val = row.get(f'Option {opt}', '')\n",
    "        if pd.notna(opt_val) and opt_val:\n",
    "            options[opt] = str(opt_val)\n",
    "            options_lines.append(f\"{opt}. {opt_val}\")\n",
    "    options_raw = \"\\n\".join(options_lines)\n",
    "    \n",
    "    # Full text for embedding\n",
    "    full_text = f\"Evidence:\\n{evidence}\\n\\nQuestion:\\n{row['Question']}\\n\\nOptions:\\n{options_raw}\\n\\nRationale:\\n{row['Rationale']}\"\n",
    "    \n",
    "    # Quality scores for EXISTING high-quality items (gold standard baseline)\n",
    "    # These are human-authored, vetted items - we assign them gold tier\n",
    "    gold_quality_scores = {\n",
    "        \"stimulus\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"stem\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"key\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"distractors\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"alignment\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"language\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"style\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []},\n",
    "        \"fairness\": {\"score\": 5, \"justification\": \"Human-authored gold standard\", \"issues\": []}\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        # Core fields\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"domain\": row.get(\"Domain\", \"\"),\n",
    "        \"topic\": row.get(\"Topic\", \"\"),\n",
    "        \"evidence\": evidence,\n",
    "        \"question_text\": row[\"Question\"],\n",
    "        \"options_raw\": options_raw,\n",
    "        \"correct_answer\": row.get(\"Answer\", \"\"),\n",
    "        \"rationale\": row.get(\"Rationale\", \"\"),\n",
    "        \"full_text\": full_text,\n",
    "        \n",
    "        # Structured components (if available, otherwise derive from question)\n",
    "        \"stimulus\": \"\",  # Original CSV may not have separate stimulus\n",
    "        \"stem\": row[\"Question\"],  # Use question as stem\n",
    "        \"option_a\": options.get(\"A\", \"\"),\n",
    "        \"option_b\": options.get(\"B\", \"\"),\n",
    "        \"option_c\": options.get(\"C\", \"\"),\n",
    "        \"option_d\": options.get(\"D\", \"\"),\n",
    "        \n",
    "        # Quality fields - GOLD TIER for existing high-quality items\n",
    "        \"quality_score\": 5.0,  # Perfect score for gold standard\n",
    "        \"quality_tier\": \"gold\",  # Highest tier\n",
    "        \"quality_summary\": \"Human-authored gold standard item from JD-Next item bank (09/2025)\",\n",
    "        \"quality_scores_json\": json.dumps(gold_quality_scores),\n",
    "        \"improvement_suggestions\": [],  # No improvements needed for gold standard\n",
    "        \n",
    "        # Review & State Management - GOLD STANDARD (no review needed)\n",
    "        \"review_status\": \"gold_standard\",  # Skips human review workflow\n",
    "        \"reviewed_at\": None,\n",
    "        \"reviewed_by\": None,\n",
    "        \"review_decision\": None,\n",
    "        \"review_explanation\": None,\n",
    "        \n",
    "        # Edit Tracking\n",
    "        \"was_edited\": False,\n",
    "        \"original_version_json\": None,\n",
    "        \"edit_summary\": None,\n",
    "        \n",
    "        # Generation Metadata (N/A for original items)\n",
    "        \"generation_batch_id\": None,\n",
    "        \"generation_attempt\": None,\n",
    "        \"similarity_at_generation\": None,\n",
    "        \"generation_metadata_json\": None,\n",
    "        \n",
    "        # Metadata\n",
    "        \"source\": \"original\",  # Distinguishes from generated items\n",
    "        \"is_generated\": False,  # Human-authored\n",
    "        \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"scored_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "\n",
    "# Convert all rows with quality metadata\n",
    "docs_v2 = [row_to_doc_v2(row) for _, row in df.iterrows()]\n",
    "print(f\"‚úÖ Converted {len(docs_v2)} documents with quality metadata\")\n",
    "print(f\"\\nSample document quality fields:\")\n",
    "print(f\"  - quality_score: {docs_v2[0]['quality_score']}\")\n",
    "print(f\"  - quality_tier: {docs_v2[0]['quality_tier']}\")\n",
    "print(f\"  - review_status: {docs_v2[0]['review_status']}\")\n",
    "print(f\"  - source: {docs_v2[0]['source']}\")\n",
    "print(f\"  - is_generated: {docs_v2[0]['is_generated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7307d8",
   "metadata": {},
   "source": [
    "### üß¨ Step 4c: Generate embeddings and upload to new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365e0ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "‚úÖ Embedded 24 documents\n",
      "‚úÖ Embedded 24 documents\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for the v2 documents\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "def embed(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=AZURE_OPENAI_EMBEDDING_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "for i, doc in enumerate(docs_v2):\n",
    "    doc[\"content_vector\"] = embed(doc[\"full_text\"])\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Embedded {i + 1}/{len(docs_v2)} documents...\")\n",
    "\n",
    "print(f\"‚úÖ Embedded {len(docs_v2)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40dfed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded batch 1: 24/24 documents\n",
      "\n",
      "üéâ Migration complete! 24 gold-standard items now in 'jdn-exam-sept2025-items'\n"
     ]
    }
   ],
   "source": [
    "# Upload to the index with quality fields\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    index_name=AZURE_EXAM_INDEX,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_KEY)\n",
    ")\n",
    "\n",
    "batch_size = 100\n",
    "for i in range(0, len(docs_v2), batch_size):\n",
    "    batch = docs_v2[i:i+batch_size]\n",
    "    result = search_client.upload_documents(documents=batch)\n",
    "    succeeded = sum(1 for r in result if r.succeeded)\n",
    "    print(f\"‚úÖ Uploaded batch {i//batch_size + 1}: {succeeded}/{len(batch)} documents\")\n",
    "\n",
    "print(f\"\\nüéâ Migration complete! {len(docs_v2)} gold-standard items now in '{AZURE_EXAM_INDEX}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fbb40",
   "metadata": {},
   "source": [
    "### ‚úÖ Step 4d: Verify the migration\n",
    "Check that quality fields are properly set and queryable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573ab17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in index: 24\n",
      "\n",
      "üìä Quality Distribution:\n",
      "  gold: 24 items\n",
      "\n",
      "üì¶ Source Distribution:\n",
      "  original: 24 items\n",
      "\n",
      "üîç Review Status Distribution:\n",
      "  gold_standard: 24 items\n",
      "\n",
      "üèÜ Sample gold-tier items:\n",
      "  - TP.2: Legal Test for Consideration: score=5.0, review_status=gold_standard\n",
      "  - TP.2: Legal Test for Consideration: score=5.0, review_status=gold_standard\n",
      "  - TP.2: Legal Test for Consideration: score=5.0, review_status=gold_standard\n",
      "  - TP.2: Legal Test for Consideration: score=5.0, review_status=gold_standard\n",
      "  - TP.2: Legal Test for Consideration: score=5.0, review_status=gold_standard\n"
     ]
    }
   ],
   "source": [
    "# Verify the migration - check quality distribution\n",
    "results = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    select=[\"id\", \"topic\", \"quality_score\", \"quality_tier\", \"source\", \"is_generated\", \"review_status\"],\n",
    "    top=1000\n",
    ")\n",
    "\n",
    "items = list(results)\n",
    "print(f\"Total items in index: {len(items)}\")\n",
    "\n",
    "# Count by quality tier, source, and review status\n",
    "tier_counts = {}\n",
    "source_counts = {}\n",
    "review_status_counts = {}\n",
    "for item in items:\n",
    "    tier = item.get(\"quality_tier\", \"unscored\")\n",
    "    source = item.get(\"source\", \"unknown\")\n",
    "    review_status = item.get(\"review_status\", \"unknown\")\n",
    "    tier_counts[tier] = tier_counts.get(tier, 0) + 1\n",
    "    source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    review_status_counts[review_status] = review_status_counts.get(review_status, 0) + 1\n",
    "\n",
    "print(f\"\\nüìä Quality Distribution:\")\n",
    "for tier, count in sorted(tier_counts.items()):\n",
    "    print(f\"  {tier}: {count} items\")\n",
    "\n",
    "print(f\"\\nüì¶ Source Distribution:\")\n",
    "for source, count in sorted(source_counts.items()):\n",
    "    print(f\"  {source}: {count} items\")\n",
    "\n",
    "print(f\"\\nüîç Review Status Distribution:\")\n",
    "for status, count in sorted(review_status_counts.items()):\n",
    "    print(f\"  {status}: {count} items\")\n",
    "\n",
    "# Test filtering by quality\n",
    "gold_items = search_client.search(\n",
    "    search_text=\"*\",\n",
    "    filter=\"quality_tier eq 'gold'\",\n",
    "    top=5\n",
    ")\n",
    "print(f\"\\nüèÜ Sample gold-tier items:\")\n",
    "for item in gold_items:\n",
    "    print(f\"  - {item['topic']}: score={item.get('quality_score', 'N/A')}, review_status={item.get('review_status', 'N/A')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
